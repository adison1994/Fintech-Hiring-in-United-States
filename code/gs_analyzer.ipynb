{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from urllib import parse as urlparse\n",
    "import re\n",
    "import csv\n",
    "\n",
    "driver = webdriver.Chrome('/Users/amivora/Downloads/chromedriver')\n",
    "target_url = 'https://globalcareers-goldmansachs.icims.com/jobs/search?pr=%s&searchCategory=44215&searchRelation=keyword_all'\n",
    "\n",
    "gs_job_title_class_name = 'title'\n",
    "gs_job_desc_class_name = 'description'\n",
    "gs_jobs = set()\n",
    "    \n",
    "driver.get(target_url % 0)\n",
    "driver.switch_to.frame(driver.find_element_by_id('icims_content_iframe'))\n",
    "last_url = driver.find_elements_by_class_name('iCIMS_Paging')[-1].find_elements_by_tag_name('a')[-1].get_attribute('href')\n",
    "parsed = urlparse.urlparse(last_url)\n",
    "last_page = urlparse.parse_qs(parsed.query)['pr'][0]\n",
    "\n",
    "for i in range(0, int(last_page) + 1):\n",
    "        target = target_url % i\n",
    "        driver.get(target)\n",
    "        driver.switch_to.frame(driver.find_element_by_id('icims_content_iframe'))\n",
    "\n",
    "        for title in driver.find_elements_by_class_name(gs_job_title_class_name):\n",
    "            for job in title.find_elements_by_tag_name('a'):\n",
    "                gs_jobs.add(job.get_attribute('href'))\n",
    "\n",
    "\n",
    "index = 0\n",
    "job_list = set()\n",
    "for job in gs_jobs:\n",
    "        try:\n",
    "            driver.get(job)\n",
    "            driver.switch_to.frame(driver.find_element_by_id('icims_content_iframe'))\n",
    "            jobdict = {}\n",
    "\n",
    "\n",
    "            for dl in driver.find_elements_by_tag_name('dl'):\n",
    "                key = dl.find_elements_by_class_name('iCIMS_JobHeaderField')[0].text\n",
    "                val = dl.find_elements_by_class_name('iCIMS_JobHeaderData')[0].text\n",
    "                jobdict[key] = val\n",
    "            \n",
    "            jobdict['url'] = job\n",
    "            jobdict['Job Title'] = driver.find_element_by_id('iCIMS_Header').text\n",
    "            jobdict['Job Responsibilities'] =  re.sub('[^A-Za-z]+', ' ', driver.find_elements_by_class_name('iCIMS_Expandable_Text')[0].text)\n",
    "            location = driver.find_elements_by_class_name('left')[0]\n",
    "            jobdict['Location'] = location.find_elements_by_tag_name('span')[1].text\n",
    "#             job_list.add(jobdict)\n",
    "            keys = jobdict.keys()\n",
    "            with open('gs_jobs.csv', 'a') as output_file:\n",
    "                dict_writer = csv.DictWriter(output_file, keys)\n",
    "                if index == 0:\n",
    "                    dict_writer.writeheader()\n",
    "                dict_writer.writerow(jobdict)\n",
    "            index = 1;\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count Goldman Sachs\n",
    "import csv\n",
    "import re\n",
    "\n",
    "with open('../deliverables/Top100_WordCount.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    list_topWords=[]\n",
    "    for row in csv_reader:\n",
    "        list_topWords.append(row[1])\n",
    "        \n",
    "with open('../deliverables/gs_jobs.csv') as csv_file_msresults:\n",
    "    csv_reader_msresults = csv.reader(csv_file_msresults, delimiter=',')\n",
    "    top_words_lowercase=[]\n",
    "    for list_top_word in list_topWords:\n",
    "        top_words_lowercase.append(list_top_word.lower())\n",
    "        \n",
    "\n",
    "    csv_prep=[]\n",
    "    for job in csv_reader_msresults:\n",
    "        job_words=[]\n",
    "        for phrase in job:\n",
    "            for word in phrase.split(' '):\n",
    "                lWord=re.sub('[^A-Za-z]+', ' ', word.lower())\n",
    "                job_words.append(lWord)\n",
    "\n",
    "        jobId=job[0]\n",
    "        jobLink=job[8]\n",
    "        listId=1\n",
    "        csvRecord={}\n",
    "        csvRecord['jobNo'] = jobId\n",
    "        csvRecord['institution'] = \"Goldman Sachs\"\n",
    "        csvRecord['url'] = jobLink\n",
    "        csvRecord['listId'] = listId\n",
    "\n",
    "        list_freq={}\n",
    "        for index, top_word in enumerate(top_words_lowercase):\n",
    "                csvRecord[index]=job_words.count(top_word)\n",
    "\n",
    "        csv_prep.append(csvRecord)\n",
    "\n",
    "keys = csv_prep[0].keys()\n",
    "with open('../deliverables/gs_web_scraped_word_count.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(csv_prep)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text rank Goldman Sachs\n",
    "import csv\n",
    "import re\n",
    "\n",
    "with open('../deliverables/Top100_TextRank.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    list_topWords=[]\n",
    "    for row in csv_reader:\n",
    "        list_topWords.append(row[1])\n",
    "        \n",
    "with open('../deliverables/gs_jobs.csv') as csv_file_msresults:\n",
    "    csv_reader_msresults = csv.reader(csv_file_msresults, delimiter=',')\n",
    "    top_words_lowercase=[]\n",
    "    for list_top_word in list_topWords:\n",
    "        top_words_lowercase.append(list_top_word.lower())\n",
    "        \n",
    "\n",
    "    csv_prep=[]\n",
    "    for job in csv_reader_msresults:\n",
    "        job_words=[]\n",
    "        for phrase in job:\n",
    "            for word in phrase.split(' '):\n",
    "                lWord=re.sub('[^A-Za-z]+', ' ', word.lower())\n",
    "                job_words.append(lWord)\n",
    "\n",
    "        jobId=job[0]\n",
    "        jobLink=job[8]\n",
    "        listId=1\n",
    "        csvRecord={}\n",
    "        csvRecord['jobNo'] = jobId\n",
    "        csvRecord['institution'] = \"Goldman Sachs\"\n",
    "        csvRecord['url'] = jobLink\n",
    "        csvRecord['listId'] = listId\n",
    "\n",
    "        list_freq={}\n",
    "        for index, top_word in enumerate(top_words_lowercase):\n",
    "                csvRecord[index]=job_words.count(top_word)\n",
    "\n",
    "        csv_prep.append(csvRecord)\n",
    "\n",
    "keys = csv_prep[0].keys()\n",
    "with open('../deliverables/gs_web_scraped_text_rank.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(csv_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text rank Goldman Sachs\n",
    "import csv\n",
    "import re\n",
    "\n",
    "with open('../deliverables/Top100_TextRank.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    list_topWords=[]\n",
    "    for row in csv_reader:\n",
    "        list_topWords.append(row[1])\n",
    "        \n",
    "with open('../deliverables/gs_jobs.csv') as csv_file_msresults:\n",
    "    csv_reader_msresults = csv.reader(csv_file_msresults, delimiter=',')\n",
    "    top_words_lowercase=[]\n",
    "    for list_top_word in list_topWords:\n",
    "        top_words_lowercase.append(list_top_word.lower())\n",
    "        \n",
    "\n",
    "    csv_prep=[]\n",
    "    for job in csv_reader_msresults:\n",
    "        job_words=[]\n",
    "        for phrase in job:\n",
    "            for word in phrase.split(' '):\n",
    "                lWord=re.sub('[^A-Za-z]+', ' ', word.lower())\n",
    "                job_words.append(lWord)\n",
    "\n",
    "        jobId=job[0]\n",
    "        jobLink=job[8]\n",
    "        listId=1\n",
    "        csvRecord={}\n",
    "        csvRecord['jobNo'] = jobId\n",
    "        csvRecord['institution'] = \"Goldman Sachs\"\n",
    "        csvRecord['url'] = jobLink\n",
    "        csvRecord['listId'] = listId\n",
    "\n",
    "        list_freq={}\n",
    "        for index, top_word in enumerate(top_words_lowercase):\n",
    "                csvRecord[index]=job_words.count(top_word)\n",
    "\n",
    "        csv_prep.append(csvRecord)\n",
    "\n",
    "keys = csv_prep[0].keys()\n",
    "with open('../deliverables/gs_web_scraped_text_rank.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(csv_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text rank TFIDF Goldman Sachs\n",
    "import csv\n",
    "import re\n",
    "\n",
    "with open('../deliverables/Top100_WordCountTFIDF.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    list_topWords=[]\n",
    "    for row in csv_reader:\n",
    "        list_topWords.append(row[1])\n",
    "        \n",
    "with open('../deliverables/gs_jobs.csv') as csv_file_msresults:\n",
    "    csv_reader_msresults = csv.reader(csv_file_msresults, delimiter=',')\n",
    "    top_words_lowercase=[]\n",
    "    for list_top_word in list_topWords:\n",
    "        top_words_lowercase.append(list_top_word.lower())\n",
    "        \n",
    "\n",
    "    csv_prep=[]\n",
    "    for job in csv_reader_msresults:\n",
    "        job_words=[]\n",
    "        for phrase in job:\n",
    "            for word in phrase.split(' '):\n",
    "                lWord=re.sub('[^A-Za-z]+', ' ', word.lower())\n",
    "                job_words.append(lWord)\n",
    "\n",
    "        jobId=job[0]\n",
    "        jobLink=job[8]\n",
    "        listId=1\n",
    "        csvRecord={}\n",
    "        csvRecord['jobNo'] = jobId\n",
    "        csvRecord['institution'] = \"Goldman Sachs\"\n",
    "        csvRecord['url'] = jobLink\n",
    "        csvRecord['listId'] = listId\n",
    "\n",
    "        list_freq={}\n",
    "        for index, top_word in enumerate(top_words_lowercase):\n",
    "                csvRecord[index]=job_words.count(top_word)\n",
    "\n",
    "        csv_prep.append(csvRecord)\n",
    "\n",
    "keys = csv_prep[0].keys()\n",
    "with open('../deliverables/gs_web_scraped_tfidf.csv', 'w') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, fieldnames=keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(csv_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
